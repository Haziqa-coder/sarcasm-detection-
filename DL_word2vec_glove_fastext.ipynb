{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_word2vec_glove_fastext.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59KQpISdM9fC",
        "outputId": "8a76351f-f0b8-4798-a127-3923411b8fad"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GRU, Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
        "from keras.initializers import Constant\n",
        "\n",
        "!pip install np_utils\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "\n",
        "# Import Plotting Libararies\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model Evaluation Libraries\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.5.12.1.tar.gz (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▍                          | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from np_utils) (0.16.0)\n",
            "Building wheels for collected packages: np-utils\n",
            "  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np-utils: filename=np_utils-0.5.12.1-py3-none-any.whl size=57131 sha256=881bf8d7e0b0128486eff0ced8f82021aef512658c7b7616fbf064dc4f0041c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/4e/ef/095c24693723c329f4cdc1079861cdbb2487d4b41b2496a4e7\n",
            "Successfully built np-utils\n",
            "Installing collected packages: np-utils\n",
            "Successfully installed np-utils-0.5.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0HUBwVONEh8"
      },
      "source": [
        "data = pd.read_csv(\"/content/sample_data/Sarcasm Dataset (1).csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyin1d0INeB_"
      },
      "source": [
        "#data = data.dropna()\n",
        "data = data.fillna(' ')\n",
        "data = data[['tweet', 'sarcastic']]\n",
        "\n",
        "X = data['tweet']\n",
        "Y = data[\"sarcastic\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip9mLcIKNeEs"
      },
      "source": [
        "import spacy\n",
        "df_w2v = data.copy() \n",
        "def tokenizer(str):\n",
        "    nlp = spacy.blank('en')\n",
        "    doc = nlp.tokenizer(str)\n",
        "    return [i.text for i in doc]\n",
        "\n",
        "df_w2v['tweet'] = df_w2v['tweet'].apply(lambda x: x.lower())\n",
        "df_w2v['tweet'] = df_w2v['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "\n",
        "for idx,row in df_w2v.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 900000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(df_w2v['tweet'].values)\n",
        "X = tokenizer.texts_to_sequences(df_w2v['tweet'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ53RDdkNeHU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.33)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EtvS3i_8Ybt",
        "outputId": "d758272f-321a-4c60-9f3f-7484f7506fc3"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2323, 63)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0DrT2DSO4Fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f816c44-764a-44d7-dc9b-e16e341c44a4"
      },
      "source": [
        "##################################### Word2Vec #################################################################\n",
        "\n",
        "sentences = [sublist.split() for sublist in data['tweet'].astype(str).values]\n",
        "words = set([item for sublist in sentences for item in sublist])\n",
        "\n",
        "import gensim\n",
        "#Dimension of vectors we are generating\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
        "w2v_model = gensim.models.Word2Vec(sentences = words , size=EMBEDDING_DIM , window = 5 , min_count = 1)\n",
        "\n",
        "len(w2v_model.wv.vocab)\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=6000)\n",
        "tokenizer.fit_on_texts(words)\n",
        "tokenized_train = tokenizer.texts_to_sequences(words)\n",
        "x = sequence.pad_sequences(tokenized_train, maxlen = 20)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size\n",
        "\n",
        "weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "def get_weight_matrix(model, vocab):\n",
        "\n",
        "  vocab_size = len(vocab) + 1\n",
        "\n",
        "  weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      weight_matrix[i] = model[word]\n",
        "  return weight_matrix\n",
        "\n",
        "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
        "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)\n",
        "\n",
        "\n",
        "#Defining Neural Network\n",
        "model1 = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model1.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=63, trainable=True))\n",
        "#LSTM \n",
        "model1.add(LSTM(units=32 , recurrent_dropout = 0.3 , dropout = 0.3))\n",
        "#model.add(Bidirectional(GRU(units=32 , recurrent_dropout = 0.1 , dropout = 0.1)))\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "#model1.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])\n",
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "del embedding_vectors\n",
        "\n",
        "model1.summary()\n",
        "\n",
        "print(\"Accuracy of the model on Training Data is - \" , model1.evaluate(X_train,y_train)[1]*100)\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model1.evaluate(X_test,y_test)[1]*100)\n",
        "\n",
        "history1 = model1.fit(X_train, y_train, epochs=2, batch_size=20)\n",
        "\n",
        "test_prediction1 = model1.predict(X_test)\n",
        "\n",
        "y_prediction = list()\n",
        "a=0\n",
        "b=1\n",
        "for val in test_prediction1:\n",
        "  if val < 0.4:\n",
        "    y_prediction.append(a)\n",
        "  else:\n",
        "    y_prediction.append(b)\n",
        "\n",
        "class_report = classification_report(y_prediction, y_test)\n",
        "conf_matrix = confusion_matrix(y_prediction , y_test)\n",
        "\n",
        "print('Results \\n')\n",
        "print(class_report)\n",
        "print(conf_matrix)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 63, 300)           3410700   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                42624     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,357\n",
            "Trainable params: 3,453,357\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "73/73 [==============================] - 2s 17ms/step - loss: 0.6957 - accuracy: 0.4731\n",
            "Accuracy of the model on Training Data is -  47.30951488018036\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.6958 - accuracy: 0.4489\n",
            "Accuracy of the model on Testing Data is -  44.89082992076874\n",
            "Epoch 1/2\n",
            "117/117 [==============================] - 19s 136ms/step - loss: 0.5740 - accuracy: 0.7486\n",
            "Epoch 2/2\n",
            "117/117 [==============================] - 16s 134ms/step - loss: 0.4544 - accuracy: 0.7766\n",
            "Results \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.76      0.73       785\n",
            "           1       0.36      0.29      0.32       360\n",
            "\n",
            "    accuracy                           0.61      1145\n",
            "   macro avg       0.53      0.53      0.53      1145\n",
            "weighted avg       0.59      0.61      0.60      1145\n",
            "\n",
            "[[598 187]\n",
            " [254 106]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci65XwapO4JE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DgFftFNNeKT",
        "outputId": "3a971ce4-1840-405b-dec6-e05a9d7fd6a9"
      },
      "source": [
        "####################################### Glove ###############################################\n",
        "\n",
        "embedding_dict={}\n",
        "with open('/content/sample_data/glove.6B.50d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "num_words=len(words)+1\n",
        "embedding_matrix=np.zeros((num_words,50))\n",
        "\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "\n",
        "X_train,X_val, y_train, y_val = train_test_split(X,Y, test_size=.2, random_state=2)\n",
        "\n",
        "model2=Sequential()\n",
        "\n",
        "embedding_layer=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix), input_length=63,trainable=False)\n",
        "\n",
        "model2.add(embedding_layer)\n",
        "\n",
        "model2.add(tf.keras.layers.LSTM(32))\n",
        "\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "print(\"Accuracy of the model on Training Data is - \" , model2.evaluate(X_train,y_train)[1]*100)\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model2.evaluate(X_test,y_test)[1]*100)\n",
        "\n",
        "\n",
        "history2 = model2.fit(X_train, y_train, epochs=2, batch_size=20)\n",
        "\n",
        "\n",
        "test_prediction2 = model2.predict(X_test)\n",
        "\n",
        "y_prediction = list()\n",
        "a=0\n",
        "b=1\n",
        "for val in test_prediction2:\n",
        "  if val < 0.4:\n",
        "    y_prediction.append(a)\n",
        "  else:\n",
        "    y_prediction.append(b)\n",
        "\n",
        "class_report = classification_report(y_prediction, y_test)\n",
        "conf_matrix = confusion_matrix(y_prediction , y_test)\n",
        "\n",
        "print('Results \\n')\n",
        "print(class_report)\n",
        "print(conf_matrix)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11368/11368 [00:00<00:00, 431048.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 63, 50)            788450    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 32)                10624     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 799,107\n",
            "Trainable params: 10,657\n",
            "Non-trainable params: 788,450\n",
            "_________________________________________________________________\n",
            "87/87 [==============================] - 2s 9ms/step - loss: 0.6356 - accuracy: 0.7203\n",
            "Accuracy of the model on Training Data is -  72.02595472335815\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.6356 - accuracy: 0.7240\n",
            "Accuracy of the model on Testing Data is -  72.40174412727356\n",
            "Epoch 1/2\n",
            "139/139 [==============================] - 6s 28ms/step - loss: 0.5733 - accuracy: 0.7487\n",
            "Epoch 2/2\n",
            "139/139 [==============================] - 4s 28ms/step - loss: 0.5627 - accuracy: 0.7495\n",
            "Results \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.74      0.85      1144\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.74      1145\n",
            "   macro avg       0.50      0.37      0.43      1145\n",
            "weighted avg       1.00      0.74      0.85      1145\n",
            "\n",
            "[[851 293]\n",
            " [  1   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul-oWrNEsyRV"
      },
      "source": [
        "##################################### Fasttext ##############################################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbVupc2eHrow",
        "outputId": "dfaa6e81-1aaf-4ef3-a822-5d312880f275"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJEIFns4H0G9",
        "outputId": "82111bae-1f04-4be1-8969-f2f6ff938fe9"
      },
      "source": [
        "!sudo mkdir ~/.kaggle\n",
        "\n",
        "!sudo cp /content/sample_data/kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wjJE94SPLRY",
        "outputId": "fa9fa5d4-9a81-4a2c-db4a-b62955f21f25"
      },
      "source": [
        "!kaggle datasets download -d yekenot/fasttext-crawl-300d-2m -p /content/kaggle"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fasttext-crawl-300d-2m.zip to /content/kaggle\n",
            " 99% 1.43G/1.44G [00:14<00:00, 105MB/s]\n",
            "100% 1.44G/1.44G [00:14<00:00, 107MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtMsvsQeSzp7",
        "outputId": "84c2fbef-1836-4f21-f145-345c0994cccc"
      },
      "source": [
        "!unzip -u '/content/kaggle/fasttext-crawl-300d-2m.zip' -d '/content/sample_data/DL_word2vec_glove_fastext'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/kaggle/fasttext-crawl-300d-2m.zip\n",
            "  inflating: /content/sample_data/DL_word2vec_glove_fastext/crawl-300d-2M.vec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0egx9hxszxTy",
        "outputId": "593052f4-813f-4fc9-834c-b7a3f06d3820"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "EMBEDDING_FILE = '/content/sample_data/DL_word2vec_glove_fastext/crawl-300d-2M.vec'\n",
        "\n",
        "embedding_dict={}\n",
        "with open(EMBEDDING_FILE ,'r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "num_words=len(words)+1\n",
        "embedding_matrix=np.zeros((num_words,300))\n",
        "\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "\n",
        "X_train,X_val, y_train, y_val = train_test_split(X,Y, test_size=.2, random_state=2)\n",
        "\n",
        "model3=Sequential()\n",
        "\n",
        "embedding_layer=Embedding(num_words,300,weights=[embedding_matrix], input_length=63,trainable=False)\n",
        "\n",
        "model3.add(embedding_layer)\n",
        "\n",
        "model3.add(tf.keras.layers.LSTM(32))\n",
        "\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "print(\"Accuracy of the model on Training Data is - \" , model3.evaluate(X_train,y_train)[1]*100)\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model3.evaluate(X_test,y_test)[1]*100)\n",
        "\n",
        "\n",
        "history3 = model3.fit(X_train, y_train, epochs=2, batch_size=20)\n",
        "\n",
        "\n",
        "test_prediction3 = model3.predict(X_test)\n",
        "\n",
        "y_prediction = list()\n",
        "a=0\n",
        "b=1\n",
        "for val in test_prediction3:\n",
        "  if val < 0.4:\n",
        "    y_prediction.append(a)\n",
        "  else:\n",
        "    y_prediction.append(b)\n",
        "\n",
        "class_report = classification_report(y_prediction, y_test)\n",
        "conf_matrix = confusion_matrix(y_prediction , y_test)\n",
        "\n",
        "print('Results \\n')\n",
        "print(class_report)\n",
        "print(conf_matrix)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11368/11368 [00:00<00:00, 409007.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 63, 300)           4730700   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 32)                42624     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,773,357\n",
            "Trainable params: 42,657\n",
            "Non-trainable params: 4,730,700\n",
            "_________________________________________________________________\n",
            "87/87 [==============================] - 2s 12ms/step - loss: 0.6920 - accuracy: 0.5260\n",
            "Accuracy of the model on Training Data is -  52.595531940460205\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.6928 - accuracy: 0.5188\n",
            "Accuracy of the model on Testing Data is -  51.87773108482361\n",
            "Epoch 1/2\n",
            "139/139 [==============================] - 7s 33ms/step - loss: 0.5767 - accuracy: 0.7469\n",
            "Epoch 2/2\n",
            "139/139 [==============================] - 5s 33ms/step - loss: 0.5541 - accuracy: 0.7495\n",
            "Results \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.74      0.85      1144\n",
            "           1       0.00      1.00      0.01         1\n",
            "\n",
            "    accuracy                           0.74      1145\n",
            "   macro avg       0.50      0.87      0.43      1145\n",
            "weighted avg       1.00      0.74      0.85      1145\n",
            "\n",
            "[[852 292]\n",
            " [  0   1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiRtTu_pjrs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id9NDu_-pjvJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}